{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単眼深度推定モデルを学習する\n",
    "このノートではいよいよ単眼深度推定モデルの学習を行う。\n",
    "ここでは簡単のために、最もシンプルなLossとネットワークをもちいて大枠の学習の方法を見ていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハイパーパラメタ\n",
    "まずはじめにハイパーパラメタを設定しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_inds = [0, -1]  # 隣接フレームの番号\n",
    "epochs = 20\n",
    "lr = 0.0001\n",
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワーク\n",
    "学習するネットワークは現在のフレームの深度を推定するものと、フレーム間の姿勢の変化を推定するものの２つである。<br>\n",
    "ここではそれぞれdepth netとpose netと呼ぶ。\n",
    "depth netは深度マップを推定するネットワークであり、pose netは回転（X軸,Y軸,Z軸）と並進（X,Y,Z）の合計6個の数値を推定するネットワークである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### depth net\n",
    "ここではネットワークの詳細に関心はないため、`segmentation_models_pytorch` をつかいU-Netを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "depth_net = smp.Unet('resnet18', in_channels=3, encoder_weights=\"imagenet\", classes=1, activation='sigmoid')\n",
    "\n",
    "# 入出力確認\n",
    "depth_net(torch.zeros(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pose net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pose netに関してもネットワークの詳細には関心がないため、torchvisionの学習済みモデルを持ってきて、部分的にレイヤーを差し替える。<br>\n",
    "pose netの入力は現在フレームと隣接フレームの２枚であり、出力はそれらのフレーム間での姿勢の変化量であるため、入力チャンネルは合計で6チャンネルとなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "pose_net = resnet18(pretrained=True)\n",
    "pose_net.conv1 = Conv2d(in_channels=3*2, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "pose_net.fc = Linear(in_features=512, out_features=6)\n",
    "\n",
    "# 入出力確認\n",
    "pose_net(torch.zeros(1, 3*2, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークの定義は以上である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss関数\n",
    "ここではPhotometric lossを定義する。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from camera import PinholeCamera\n",
    "from functools import lru_cache\n",
    "\n",
    "class PhotometricLoss(torch.nn.Module):\n",
    "    def __init__(self, frame_inds):\n",
    "        super().__init__()\n",
    "        self.frame_inds = frame_inds\n",
    "        self.l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        image_target = y[\"rgb_0\"]\n",
    "        depth = y_pred[f\"depth_0\"]\n",
    "        intrinsic = y[\"intrinsic_0\"]\n",
    "        loss = 0\n",
    "        for idx in self.frame_inds:\n",
    "            # target 同士は比較しない\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            image_source = y[f\"rgb_{idx}\"]\n",
    "            extrinsic_src2tgt = y_pred[f\"extrinsic_{idx}\"]\n",
    "            image_warped = self.warp(image_source, depth, intrinsic, extrinsic_src2tgt)\n",
    "            loss += self.l1_loss(image_warped, image_target)\n",
    "        return loss, image_warped\n",
    "\n",
    "    def warp(self, image_source, depth, intrinsic, extrinsic_src2tgt):\n",
    "        image_coords = self.create_image_coords(depth.shape)\n",
    "        image_coords = image_coords.to(depth.device)\n",
    "        world_coords = PinholeCamera.image2world(image_coords, intrinsic, extrinsic_src2tgt, depth, batch=True)\n",
    "        # skip world2camera() because the coord is already camera coords here.\n",
    "        world_coords = world_coords[..., :3] # remove 4th dim        \n",
    "        image_coords = PinholeCamera.camera2image(world_coords, intrinsic, batch=True)        \n",
    "        # normalize the coord to be in [-1, 1] for grid sampling\n",
    "        image_coords[..., 0] = image_coords[..., 0] / image_coords.shape[2] * 2 - 1\n",
    "        image_coords[..., 1] = image_coords[..., 1] / image_coords.shape[1] * 2 - 1\n",
    "        grid = image_coords\n",
    "        image_warped = F.grid_sample(image_source, grid)\n",
    "        return image_warped\n",
    "                \n",
    "    @lru_cache(None)\n",
    "    def create_image_coords(self, map_shape):\n",
    "        # 以前と同様にmeshgridで画像座標を生成する\n",
    "        xi = torch.arange(0, map_shape[2], 1)\n",
    "        yi = torch.arange(0, map_shape[1], 1)\n",
    "        coord_x, coord_y = torch.meshgrid(xi, yi, indexing=\"xy\")\n",
    "        image_coords = torch.stack([coord_x, coord_y], axis=-1)\n",
    "        image_coords = image_coords.float()\n",
    "        # batch\n",
    "        image_coords = image_coords.unsqueeze(0).repeat(map_shape[0], 1, 1, 1)\n",
    "        return image_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = PhotometricLoss(frame_inds=frame_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Utils\n",
    "pose netが推定した姿勢の変化量（6変数）を4x4の行列に変換するユーティリティ関数を定義する。\n",
    "と思ったが、torchgeometryで提供されている関数が便利だったため、それを使うことにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchgeometry as tgm\n",
    "\n",
    "tgm.rtvec_to_pose(torch.rand(3, 6)).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import VKITTI2\n",
    "\n",
    "train_dataset = VKITTI2(root_dir=\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "class Transform(Dataset):\n",
    "    def __init__(self, dataset):        \n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.crop = (45, 43, 1197, 331) # (x0, y0, x1, y1)\n",
    "        self.scale = 1.0 / 3.0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        for key in data.keys():\n",
    "            if key.startswith(\"rgb_\") or key.startswith(\"depth_\"):\n",
    "                image = data[key]\n",
    "                image = image[self.crop[1]:self.crop[3], self.crop[0]:self.crop[2]]\n",
    "                orig_shape = image.shape\n",
    "                dest_size = (int(orig_shape[1] * self.scale), int(orig_shape[0] * self.scale))\n",
    "                image = cv2.resize(image, dest_size) \n",
    "                data[key] = torch.tensor(image).float()\n",
    "                if key.startswith(\"rgb_\"):\n",
    "                    data[key] = data[key].permute(2, 0, 1) # (B, H, W, C) -> (B, C, H, W)\n",
    "                    data[key] /= 255.0 # normalize\n",
    "            elif key.startswith(\"intrinsic_\"):\n",
    "                intrinsic = data[key]\n",
    "                # apply crop offset and scale\n",
    "                intrinsic[0, 2] = intrinsic[0, 2] - self.crop[0]\n",
    "                intrinsic[1, 2] = intrinsic[1, 2] - self.crop[1]\n",
    "                intrinsic[:2, :] *= self.scale\n",
    "                data[key] = intrinsic\n",
    "            \n",
    "            data[key] = torch.tensor(data[key]).float()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(Transform(train_dataset), batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "device=\"cuda:0\"\n",
    "\n",
    "depth_net.train().to(device)\n",
    "pose_net.train().to(device)\n",
    "\n",
    "optimizer = Adam([\n",
    "    {\"params\": depth_net.parameters()},\n",
    "    {\"params\": pose_net.parameters()}],\n",
    "    lr=lr)\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    with tqdm(train_dataloader) as pbar:\n",
    "        for batch in pbar:\n",
    "            # transport batch to device\n",
    "            batch = {k:v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # predict depth\n",
    "            inv_depth = depth_net(batch[\"rgb_0\"])\n",
    "            inv_depth = inv_depth.squeeze(1)\n",
    "            inv_depth = torch.clip(inv_depth, min=0.001, max=100)\n",
    "            depth = 1 / inv_depth\n",
    "\n",
    "            # precict pose\n",
    "            image_concat = torch.cat([batch[\"rgb_0\"], batch[\"rgb_-1\"]], axis=1)\n",
    "            pose = pose_net(image_concat)\n",
    "            rtmat = tgm.rtvec_to_pose(pose)\n",
    "\n",
    "            # compute loss\n",
    "            y = {k:v for k, v in batch.items() if k.startswith(\"rgb_\") or k.startswith(\"intrinsic_\")}\n",
    "            y_pred = {\n",
    "                \"depth_0\": depth,\n",
    "                \"extrinsic_-1\": rtmat,\n",
    "            }\n",
    "            loss, image_warped = criterion(y, y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # status update\n",
    "            pbar.set_description(f\"[Epoch {i}] loss: {loss:0.3f}, depth mean {depth.mean():0.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cbcaf0c4fc7822c0e687e8f5e219a9e13219a8a0c31d9376a0b5fe7b34ec9a8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
